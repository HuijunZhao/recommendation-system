{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses deep learning (CNN, GRU, LSTM) to predict user ratings of movies based on user reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huiju\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>I highly recommend this series. It is a must f...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>This one is a real snoozer. Don't believe anyt...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>Mysteries are interesting.  The tension betwee...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                                         reviewText  \\\n",
       "0  A11N155CW1UV02  I had big expectations because I love English ...   \n",
       "1  A3BC8O2KCL29V2  I highly recommend this series. It is a must f...   \n",
       "2   A60D5HQFOTSOM  This one is a real snoozer. Don't believe anyt...   \n",
       "3  A1RJPIGRSNX4PW  Mysteries are interesting.  The tension betwee...   \n",
       "4  A16XRPF40679KG  This show always is excellent, as far as briti...   \n",
       "\n",
       "         asin  overall  \n",
       "0  B000H00VBQ      2.0  \n",
       "1  B000H00VBQ      5.0  \n",
       "2  B000H00VBQ      1.0  \n",
       "3  B000H00VBQ      4.0  \n",
       "4  B000H00VBQ      5.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_list_of_dicts(fname): \n",
    "    return [json.loads(i) for i in open(fname, \"rt\")]\n",
    "\n",
    "raw_data = get_list_of_dicts(\"Amazon_Instant_Video_5.json\")\n",
    "data = pd.DataFrame(raw_data).loc[:, [\"reviewerID\", \"reviewText\", \"asin\", \"overall\"]]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I really like the characters and the actors. I...</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is one good show. It is interesting in it...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I watched this a couple of weeks ago. There ar...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The acting was excellent.  The acting, the rel...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As many people said this show kept getting bet...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  I really like the characters and the actors. I...   \n",
       "1  This is one good show. It is interesting in it...   \n",
       "2  I watched this a couple of weeks ago. There ar...   \n",
       "3  The acting was excellent.  The acting, the rel...   \n",
       "4  As many people said this show kept getting bet...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  This show always is excellent, as far as briti...  \n",
       "1  I had big expectations because I love English ...  \n",
       "2  I had big expectations because I love English ...  \n",
       "3  I had big expectations because I love English ...  \n",
       "4  I had big expectations because I love English ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_user_reviews(x):\n",
    "    ur = user_reviews.loc[x[\"reviewerID\"]].drop(x[\"asin\"]).values.tolist()\n",
    "    mr = movie_reviews.loc[x[\"asin\"]].drop(x[\"reviewerID\"]).values.tolist()\n",
    "    x[\"userReviews\"] = \" \".join(list(map(lambda x: x[0], ur)))\n",
    "    x[\"movieReviews\"] = \" \".join(list(map(lambda x: x[0], mr)))\n",
    "    return x\n",
    "\n",
    "user_item_review = data.drop(\"reviewText\", axis=1)\n",
    "user_reviews = pd.pivot_table(data, index=[\"reviewerID\", \"asin\"], aggfunc=lambda x: x).drop(\"overall\", axis=1)  \n",
    "movie_reviews = pd.pivot_table(data, index=[\"asin\", \"reviewerID\"], aggfunc=lambda x: x).drop(\"overall\", axis=1)\n",
    "\n",
    "df = user_item_review.apply(add_user_reviews, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I really like the characters and the actors. I...</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is one good show. It is interesting in it...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I watched this a couple of weeks ago. There ar...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The acting was excellent.  The acting, the rel...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As many people said this show kept getting bet...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  I really like the characters and the actors. I...   \n",
       "1  This is one good show. It is interesting in it...   \n",
       "2  I watched this a couple of weeks ago. There ar...   \n",
       "3  The acting was excellent.  The acting, the rel...   \n",
       "4  As many people said this show kept getting bet...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  This show always is excellent, as far as briti...  \n",
       "1  I had big expectations because I love English ...  \n",
       "2  I had big expectations because I love English ...  \n",
       "3  I had big expectations because I love English ...  \n",
       "4  I had big expectations because I love English ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test split \n",
    "test_size = 0.005\n",
    "\n",
    "# get test_size percentage of users\n",
    "unique_users = df.loc[:, \"reviewerID\"].unique()\n",
    "users_size = len(unique_users)\n",
    "test_idx = np.random.choice(users_size, size=int(users_size * test_size), replace=False)\n",
    "\n",
    "# get test users\n",
    "test_users = unique_users[test_idx]\n",
    "\n",
    "# everyone else is a training user\n",
    "train_users = np.delete(unique_users, test_idx)\n",
    "\n",
    "test = df[df[\"reviewerID\"].isin(test_users)]\n",
    "train = df[df[\"reviewerID\"].isin(train_users)]\n",
    "\n",
    "unique_test_movies = test[\"asin\"].unique()\n",
    "\n",
    "# drop the movies that also appear in our test set. In order to be\n",
    "# a true train/test split, we are forced to discard some data entirely\n",
    "train = train.where(np.logical_not(train[\"asin\"].isin(unique_test_movies))).dropna()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the reviews into GloVe word2vect model. \n",
    "\n",
    "The pre-trained GloVe model is downloadable at\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path\n",
    "# functions to embed user reviews into the GloVe word2vect model\n",
    "def init_embeddings_map(fname):\n",
    "    with open(os.path.join(\"glove.6B\", fname), encoding=\"utf8\") as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in\n",
    "                [line.split() for line in glove]}\n",
    "\n",
    "def get_embed_func(i_len, u_len, pad_value, embedding_map):\n",
    "    def embed(row):\n",
    "        sentence = row[\"userReviews\"].split()[:u_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word)\n",
    "            if word in embedding_map else pad_value, sentence))\n",
    "        row[\"userReviews\"] = reviews +[pad_value] * (u_len - len(reviews))\n",
    "        sentence = row[\"movieReviews\"].split()[:i_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word) if word in embedding_map else pad_value, sentence))\n",
    "        row[\"movieReviews\"] = reviews +[pad_value] * (i_len - len(reviews))\n",
    "        return row\n",
    "    return embed\n",
    "\n",
    "emb_size = 50 #or 100, 200, 300\n",
    "embedding_map = init_embeddings_map(\"glove.6B.\" + str(emb_size) + \"d.txt\")\n",
    "\n",
    "user_sizes = df.loc[:, \"userReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "item_sizes = df.loc[:, \"movieReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "\n",
    "u_ptile = 40\n",
    "i_ptile = 15\n",
    "u_len = int(np.percentile(user_sizes, u_ptile))\n",
    "i_len = int(np.percentile(item_sizes, i_ptile))\n",
    "\n",
    "embedding_fn = get_embed_func(i_len, u_len, np.array([0.0] * emb_size), embedding_map)\n",
    "    \n",
    "train_embedded = train.apply(embedding_fn, axis=1)\n",
    "test_embedded = test.apply(embedding_fn, axis=1)\n",
    "\n",
    "print(u_len, i_len) # size of input in deep neural networks, useful to set parameters\n",
    "train_embedded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Conv1D, GRU, LSTM, MaxPooling1D, Flatten\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.merge import Add, Dot, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 241, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 722, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 232, 4)       2004        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 713, 4)       2004        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 116, 4)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 356, 4)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 107, 4)       164         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 347, 4)       164         max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 53, 4)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 173, 4)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 212)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 692)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           13632       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           44352       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dense_3[0][0]                    \n",
      "                                                                 dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 62,449\n",
      "Trainable params: 62,449\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_tower(max_len, embedding_size, hidden_size, filters=4, kernel_size=10):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = Conv1D(filters=filters, kernel_size=kernel_size, activation=\"tanh\")(input_layer)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Conv1D(filters=filters, kernel_size=kernel_size, activation=\"tanh\")(tower)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def CNN_model(embedding_size, hidden_size, u_len, i_len):\n",
    "    inputU, towerU = cnn_tower(u_len, embedding_size, hidden_size)\n",
    "    inputM, towerM = cnn_tower(i_len, embedding_size, hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "hidden_size = 64\n",
    "\n",
    "model_cnn = CNN_model(emb_size, hidden_size, u_len, i_len)\n",
    "model_cnn.compile(optimizer='Adam', loss='mse')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24469 samples, validate on 1288 samples\n",
      "Epoch 1/20\n",
      "24469/24469 [==============================] - 202s 8ms/step - loss: 1.6914 - val_loss: 1.6619\n",
      "Epoch 2/20\n",
      "24469/24469 [==============================] - 198s 8ms/step - loss: 1.3834 - val_loss: 1.6515\n",
      "Epoch 3/20\n",
      "24469/24469 [==============================] - 200s 8ms/step - loss: 1.2819 - val_loss: 1.4857\n",
      "Epoch 4/20\n",
      "24469/24469 [==============================] - 202s 8ms/step - loss: 1.2089 - val_loss: 1.4324\n",
      "Epoch 5/20\n",
      "24469/24469 [==============================] - 198s 8ms/step - loss: 1.1670 - val_loss: 1.4497\n",
      "Epoch 6/20\n",
      "24469/24469 [==============================] - 203s 8ms/step - loss: 1.1299 - val_loss: 1.4858\n",
      "Epoch 7/20\n",
      "24469/24469 [==============================] - 205s 8ms/step - loss: 1.0987 - val_loss: 1.5414\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "user_reviews = np.array(list(train_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(train_embedded.loc[:, \"movieReviews\"]))\n",
    "\n",
    "train_inputs = [user_reviews, movie_reviews]\n",
    "train_outputs = train_embedded.loc[:, \"overall\"]\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"cnn_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('cnn_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_cnn.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_cnn.save(\"cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 241, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 722, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 64)           22080       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 64)           22080       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            129         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1)            0           dense_6[0][0]                    \n",
      "                                                                 dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 52,609\n",
      "Trainable params: 52,609\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def gru_tower(max_len, embedding_size, hidden_size, rnn_hidden_size, filters=2, kernel_size=8):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = GRU(rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def GRU_model(embedding_size, hidden_size, rnn_hidden_size, u_len, i_len):\n",
    "    inputU, towerU = gru_tower(u_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    inputM, towerM = gru_tower(i_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "model_gru = GRU_model(emb_size, hidden_size, rnn_hidden_size, u_len, i_len)\n",
    "model_gru.compile(optimizer='Adam', loss='mse')\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24469 samples, validate on 1288 samples\n",
      "Epoch 1/20\n",
      "24469/24469 [==============================] - 450s 18ms/step - loss: 1.9215 - val_loss: 1.4802\n",
      "Epoch 2/20\n",
      "24469/24469 [==============================] - 440s 18ms/step - loss: 1.3437 - val_loss: 1.5218\n",
      "Epoch 3/20\n",
      "24469/24469 [==============================] - 456s 19ms/step - loss: 1.2979 - val_loss: 1.4862\n",
      "Epoch 4/20\n",
      "24469/24469 [==============================] - 479s 20ms/step - loss: 1.2170 - val_loss: 1.4137\n",
      "Epoch 5/20\n",
      "24469/24469 [==============================] - 493s 20ms/step - loss: 1.1893 - val_loss: 1.3837\n",
      "Epoch 6/20\n",
      "24469/24469 [==============================] - 488s 20ms/step - loss: 1.1457 - val_loss: 1.3981\n",
      "Epoch 7/20\n",
      "24469/24469 [==============================] - 458s 19ms/step - loss: 1.1135 - val_loss: 1.3831\n",
      "Epoch 8/20\n",
      "24469/24469 [==============================] - 479s 20ms/step - loss: 1.0895 - val_loss: 1.4054\n",
      "Epoch 9/20\n",
      "24469/24469 [==============================] - 469s 19ms/step - loss: 1.0559 - val_loss: 1.3657\n",
      "Epoch 10/20\n",
      "24469/24469 [==============================] - 470s 19ms/step - loss: 1.0167 - val_loss: 1.4891\n",
      "Epoch 11/20\n",
      "24469/24469 [==============================] - 467s 19ms/step - loss: 0.9980 - val_loss: 1.3611\n",
      "Epoch 12/20\n",
      "24469/24469 [==============================] - 473s 19ms/step - loss: 0.9620 - val_loss: 1.4182\n",
      "Epoch 13/20\n",
      "24469/24469 [==============================] - 465s 19ms/step - loss: 0.9453 - val_loss: 1.3904\n",
      "Epoch 14/20\n",
      "24469/24469 [==============================] - 476s 19ms/step - loss: 0.9056 - val_loss: 1.4037\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"gru_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('gru_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_gru.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_gru.save(\"gru.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 241, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 722, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           29440       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 64)           29440       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           4160        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            129         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_9[0][0]                    \n",
      "                                                                 dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 67,329\n",
      "Trainable params: 67,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "def lstm_tower(max_len, embedding_size, hidden_size, rnn_hidden_size, filters=2, kernel_size=8):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = LSTM(rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def LSTM_model(embedding_size, hidden_size, rnn_hidden_size, u_len, i_len):\n",
    "    inputU, towerU = lstm_tower(u_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    inputM, towerM = lstm_tower(i_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "\n",
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "model_lstm = LSTM_model(emb_size, hidden_size, rnn_hidden_size, u_len, i_len)\n",
    "model_lstm.compile(optimizer='Adam', loss='mse')\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24469 samples, validate on 1288 samples\n",
      "Epoch 1/30\n",
      "24469/24469 [==============================] - 560s 23ms/step - loss: 0.9804 - val_loss: 1.3751\n",
      "Epoch 2/30\n",
      "24469/24469 [==============================] - 552s 23ms/step - loss: 0.9548 - val_loss: 1.4071\n",
      "Epoch 3/30\n",
      "24469/24469 [==============================] - 569s 23ms/step - loss: 0.9279 - val_loss: 1.5415\n",
      "Epoch 4/30\n",
      "24469/24469 [==============================] - 577s 24ms/step - loss: 0.9088 - val_loss: 1.4098\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"lstm_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('lstm_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_lstm.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_lstm.save(\"lstm.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE for CNN model : 1.606608528314209\n",
      "Test MSE for GRU model : 1.5976189643735184\n",
      "Test MSE for LSTM model : 1.5789539424967107\n"
     ]
    }
   ],
   "source": [
    "user_reviews = np.array(list(test_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(test_embedded.loc[:, \"movieReviews\"]))\n",
    "test_inputs = [user_reviews, movie_reviews]\n",
    "\n",
    "true_rating = np.array(list(test_embedded.loc[:, \"overall\"])).reshape((-1, 1))\n",
    "\n",
    "predictions_cnn = model_cnn.predict(test_inputs)\n",
    "predictions_gru = model_gru.predict(test_inputs)\n",
    "predictions_lstm = model_lstm.predict(test_inputs)\n",
    "\n",
    "error_cnn = np.square(predictions_cnn - true_rating)\n",
    "print(\"Test MSE for CNN model :\", np.average(error_cnn))\n",
    "\n",
    "error_gru = np.square(predictions_gru - true_rating)\n",
    "print(\"Test MSE for GRU model :\", np.average(error_gru))\n",
    "\n",
    "error_lstm = np.square(predictions_lstm - true_rating)\n",
    "print(\"Test MSE for LSTM model :\", np.average(error_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is very sparse and we need to run more epochs, and also a larger dataset. So the resulting MSE's are not so satisfying. However, we can still compare them and draw some early conclusions.\n",
    "\n",
    "1. RNN works better than CNN. A possible reason might be that reviews are sequential data.\n",
    "2. LSTM works better than GRU. More epochs will lead to better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
