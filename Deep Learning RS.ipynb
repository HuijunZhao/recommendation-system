{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses deep learning (CNN, GRU, LSTM) to predict user ratings of movies based on user reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huiju\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>I highly recommend this series. It is a must f...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>This one is a real snoozer. Don't believe anyt...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>Mysteries are interesting.  The tension betwee...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                                         reviewText  \\\n",
       "0  A11N155CW1UV02  I had big expectations because I love English ...   \n",
       "1  A3BC8O2KCL29V2  I highly recommend this series. It is a must f...   \n",
       "2   A60D5HQFOTSOM  This one is a real snoozer. Don't believe anyt...   \n",
       "3  A1RJPIGRSNX4PW  Mysteries are interesting.  The tension betwee...   \n",
       "4  A16XRPF40679KG  This show always is excellent, as far as briti...   \n",
       "\n",
       "         asin  overall  \n",
       "0  B000H00VBQ      2.0  \n",
       "1  B000H00VBQ      5.0  \n",
       "2  B000H00VBQ      1.0  \n",
       "3  B000H00VBQ      4.0  \n",
       "4  B000H00VBQ      5.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_list_of_dicts(fname): \n",
    "    return [json.loads(i) for i in open(fname, \"rt\")]\n",
    "\n",
    "raw_data = get_list_of_dicts(\"Amazon_Instant_Video_5.json\")\n",
    "data = pd.DataFrame(raw_data).loc[:, [\"reviewerID\", \"reviewText\", \"asin\", \"overall\"]]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I really like the characters and the actors. I...</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is one good show. It is interesting in it...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I watched this a couple of weeks ago. There ar...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The acting was excellent.  The acting, the rel...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As many people said this show kept getting bet...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  I really like the characters and the actors. I...   \n",
       "1  This is one good show. It is interesting in it...   \n",
       "2  I watched this a couple of weeks ago. There ar...   \n",
       "3  The acting was excellent.  The acting, the rel...   \n",
       "4  As many people said this show kept getting bet...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  This show always is excellent, as far as briti...  \n",
       "1  I had big expectations because I love English ...  \n",
       "2  I had big expectations because I love English ...  \n",
       "3  I had big expectations because I love English ...  \n",
       "4  I had big expectations because I love English ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_user_reviews(x):\n",
    "    ur = user_reviews.loc[x[\"reviewerID\"]].drop(x[\"asin\"]).values.tolist()\n",
    "    mr = movie_reviews.loc[x[\"asin\"]].drop(x[\"reviewerID\"]).values.tolist()\n",
    "    x[\"userReviews\"] = \" \".join(list(map(lambda x: x[0], ur)))\n",
    "    x[\"movieReviews\"] = \" \".join(list(map(lambda x: x[0], mr)))\n",
    "    return x\n",
    "\n",
    "user_item_review = data.drop(\"reviewText\", axis=1)\n",
    "user_reviews = pd.pivot_table(data, index=[\"reviewerID\", \"asin\"], aggfunc=lambda x: x).drop(\"overall\", axis=1)  \n",
    "movie_reviews = pd.pivot_table(data, index=[\"asin\", \"reviewerID\"], aggfunc=lambda x: x).drop(\"overall\", axis=1)\n",
    "\n",
    "df = user_item_review.apply(add_user_reviews, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I really like the characters and the actors. I...</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This is one good show. It is interesting in it...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I watched this a couple of weeks ago. There ar...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The acting was excellent.  The acting, the rel...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As many people said this show kept getting bet...</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  I really like the characters and the actors. I...   \n",
       "1  This is one good show. It is interesting in it...   \n",
       "2  I watched this a couple of weeks ago. There ar...   \n",
       "3  The acting was excellent.  The acting, the rel...   \n",
       "4  As many people said this show kept getting bet...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  This show always is excellent, as far as briti...  \n",
       "1  I had big expectations because I love English ...  \n",
       "2  I had big expectations because I love English ...  \n",
       "3  I had big expectations because I love English ...  \n",
       "4  I had big expectations because I love English ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test split \n",
    "test_size = 0.005\n",
    "\n",
    "# get test_size percentage of users\n",
    "unique_users = df.loc[:, \"reviewerID\"].unique()\n",
    "users_size = len(unique_users)\n",
    "test_idx = np.random.choice(users_size, size=int(users_size * test_size), replace=False)\n",
    "\n",
    "# get test users\n",
    "test_users = unique_users[test_idx]\n",
    "\n",
    "# everyone else is a training user\n",
    "train_users = np.delete(unique_users, test_idx)\n",
    "\n",
    "test = df[df[\"reviewerID\"].isin(test_users)]\n",
    "train = df[df[\"reviewerID\"].isin(train_users)]\n",
    "\n",
    "unique_test_movies = test[\"asin\"].unique()\n",
    "\n",
    "# drop the movies that also appear in our test set. In order to be\n",
    "# a true train/test split, we are forced to discard some data entirely\n",
    "train = train.where(np.logical_not(train[\"asin\"].isin(unique_test_movies))).dropna()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the reviews into GloVe word2vect model. \n",
    "\n",
    "The pre-trained GloVe model is downloadable at\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>userReviews</th>\n",
       "      <th>movieReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ      2.0   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ      5.0   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ      1.0   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ      4.0   \n",
       "4  A16XRPF40679KG  B000H00VBQ      5.0   \n",
       "\n",
       "                                         userReviews  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                        movieReviews  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path\n",
    "# functions to embed user reviews into the GloVe word2vect model\n",
    "def init_embeddings_map(fname):\n",
    "    with open(os.path.join(\"glove.6B\", fname), encoding=\"utf8\") as glove:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in\n",
    "                [line.split() for line in glove]}\n",
    "\n",
    "def get_embed_func(i_len, u_len, pad_value, embedding_map):\n",
    "    def embed(row):\n",
    "        sentence = row[\"userReviews\"].split()[:u_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word)\n",
    "            if word in embedding_map else pad_value, sentence))\n",
    "        row[\"userReviews\"] = reviews +[pad_value] * (u_len - len(reviews))\n",
    "        sentence = row[\"movieReviews\"].split()[:i_len]\n",
    "        reviews = list(map(lambda word: embedding_map.get(word) if word in embedding_map else pad_value, sentence))\n",
    "        row[\"movieReviews\"] = reviews +[pad_value] * (i_len - len(reviews))\n",
    "        return row\n",
    "    return embed\n",
    "\n",
    "emb_size = 50 #or 100, 200, 300\n",
    "embedding_map = init_embeddings_map(\"glove.6B.\" + str(emb_size) + \"d.txt\")\n",
    "\n",
    "user_sizes = df.loc[:, \"userReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "item_sizes = df.loc[:, \"movieReviews\"].apply(lambda x: x.split()).apply(len)\n",
    "\n",
    "u_ptile = 40\n",
    "i_ptile = 15\n",
    "u_len = int(np.percentile(user_sizes, u_ptile))\n",
    "i_len = int(np.percentile(item_sizes, i_ptile))\n",
    "\n",
    "embedding_fn = get_embed_func(i_len, u_len, np.array([0.0] * emb_size), embedding_map)\n",
    "    \n",
    "train_embedded = train.apply(embedding_fn, axis=1)\n",
    "test_embedded = test.apply(embedding_fn, axis=1)\n",
    "\n",
    "print(u_len, i_len) # size of input in deep neural networks, useful to set parameters\n",
    "train_embedded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Conv1D, GRU, LSTM, MaxPooling1D, Flatten\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.merge import Add, Dot, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 241, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 722, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 232, 4)       2004        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 713, 4)       2004        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 116, 4)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 356, 4)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 107, 4)       164         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 347, 4)       164         max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 53, 4)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 173, 4)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 212)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 692)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           13632       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           44352       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1)            0           dense_3[0][0]                    \n",
      "                                                                 dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 62,449\n",
      "Trainable params: 62,449\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cnn_tower(max_len, embedding_size, hidden_size, filters=4, kernel_size=10):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = Conv1D(filters=filters, kernel_size=kernel_size, activation=\"tanh\")(input_layer)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Conv1D(filters=filters, kernel_size=kernel_size, activation=\"tanh\")(tower)\n",
    "        tower = MaxPooling1D()(tower)\n",
    "        tower = Flatten()(tower)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def CNN_model(embedding_size, hidden_size, u_len, i_len):\n",
    "    inputU, towerU = cnn_tower(u_len, embedding_size, hidden_size)\n",
    "    inputM, towerM = cnn_tower(i_len, embedding_size, hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "hidden_size = 64\n",
    "\n",
    "model_cnn = CNN_model(emb_size, hidden_size, u_len, i_len)\n",
    "model_cnn.compile(optimizer='Adam', loss='mse')\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "user_reviews = np.array(list(train_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(train_embedded.loc[:, \"movieReviews\"]))\n",
    "\n",
    "train_inputs = [user_reviews, movie_reviews]\n",
    "train_outputs = train_embedded.loc[:, \"overall\"]\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"cnn_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('cnn_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_cnn.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_cnn.save(\"cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 241, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 722, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 64)           22080       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 64)           22080       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            129         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1)            0           dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1)            0           dense_6[0][0]                    \n",
      "                                                                 dot_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 52,609\n",
      "Trainable params: 52,609\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def gru_tower(max_len, embedding_size, hidden_size, rnn_hidden_size, filters=2, kernel_size=8):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = GRU(rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def GRU_model(embedding_size, hidden_size, rnn_hidden_size, u_len, i_len):\n",
    "    inputU, towerU = gru_tower(u_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    inputM, towerM = gru_tower(i_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "model_gru = GRU_model(emb_size, hidden_size, rnn_hidden_size, u_len, i_len)\n",
    "model_gru.compile(optimizer='Adam', loss='mse')\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25841 samples, validate on 1361 samples\n",
      "Epoch 1/10\n",
      "25841/25841 [==============================] - 493s 19ms/step - loss: 1.8950 - val_loss: 1.6200\n",
      "Epoch 2/10\n",
      "25841/25841 [==============================] - 487s 19ms/step - loss: 1.3385 - val_loss: 1.5022\n",
      "Epoch 3/10\n",
      "25841/25841 [==============================] - 518s 20ms/step - loss: 1.2699 - val_loss: 1.4920\n",
      "Epoch 4/10\n",
      "25841/25841 [==============================] - 495s 19ms/step - loss: 1.2187 - val_loss: 1.4732\n",
      "Epoch 5/10\n",
      "25841/25841 [==============================] - 491s 19ms/step - loss: 1.1813 - val_loss: 1.4453\n",
      "Epoch 6/10\n",
      "25841/25841 [==============================] - 510s 20ms/step - loss: 1.1526 - val_loss: 1.4550\n",
      "Epoch 7/10\n",
      "25841/25841 [==============================] - 506s 20ms/step - loss: 1.1185 - val_loss: 1.4365\n",
      "Epoch 8/10\n",
      "25841/25841 [==============================] - 504s 20ms/step - loss: 1.0878 - val_loss: 1.4346\n",
      "Epoch 9/10\n",
      "25841/25841 [==============================] - 505s 20ms/step - loss: 1.0384 - val_loss: 1.4366\n",
      "Epoch 10/10\n",
      "25841/25841 [==============================] - 501s 19ms/step - loss: 1.0046 - val_loss: 1.4314\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"gru_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('gru_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_gru.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_gru.save(\"gru.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 241, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 722, 50)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 64)           29440       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 64)           29440       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           4160        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 64)           4160        lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            129         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1)            0           dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1)            0           dense_10[0][0]                   \n",
      "                                                                 dot_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 67,329\n",
      "Trainable params: 67,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "def lstm_tower(max_len, embedding_size, hidden_size, rnn_hidden_size, filters=2, kernel_size=8):\n",
    "        input_layer = Input(shape=(max_len, embedding_size))\n",
    "        tower = LSTM(rnn_hidden_size, activation=\"tanh\")(input_layer)\n",
    "        tower = Dense(hidden_size, activation=\"relu\")(tower)\n",
    "        tower = Dropout(0.4)(tower)\n",
    "        return input_layer, tower\n",
    "    \n",
    "def LSTM_model(embedding_size, hidden_size, rnn_hidden_size, u_len, i_len):\n",
    "    inputU, towerU = lstm_tower(u_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    inputM, towerM = lstm_tower(i_len, embedding_size, hidden_size, rnn_hidden_size)\n",
    "    joined = Concatenate()([towerU, towerM])\n",
    "    outNeuron = Dense(1)(joined)\n",
    "    dotproduct = Dot(axes=1)([towerU, towerM])\n",
    "    output_layer = Add()([outNeuron, dotproduct])\n",
    "        \n",
    "    model = Model(inputs=[inputU, inputM], outputs=[output_layer])\n",
    "    return model\n",
    "\n",
    "\n",
    "hidden_size = 64\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "model_lstm = LSTM_model(emb_size, hidden_size, rnn_hidden_size, u_len, i_len)\n",
    "model_lstm.compile(optimizer='Adam', loss='mse')\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25841 samples, validate on 1361 samples\n",
      "Epoch 1/20\n",
      "25841/25841 [==============================] - 585s 23ms/step - loss: 1.7464 - val_loss: 1.6312\n",
      "Epoch 2/20\n",
      "25841/25841 [==============================] - 599s 23ms/step - loss: 1.4380 - val_loss: 1.6305\n",
      "Epoch 3/20\n",
      "25841/25841 [==============================] - 592s 23ms/step - loss: 1.3742 - val_loss: 1.5385\n",
      "Epoch 4/20\n",
      "25841/25841 [==============================] - 604s 23ms/step - loss: 1.2837 - val_loss: 1.5487\n",
      "Epoch 5/20\n",
      "25841/25841 [==============================] - 588s 23ms/step - loss: 1.2562 - val_loss: 1.5626\n",
      "Epoch 6/20\n",
      "25841/25841 [==============================] - 589s 23ms/step - loss: 1.2001 - val_loss: 1.5301\n",
      "Epoch 7/20\n",
      "25841/25841 [==============================] - 600s 23ms/step - loss: 1.1804 - val_loss: 1.5274\n",
      "Epoch 8/20\n",
      "25841/25841 [==============================] - 606s 23ms/step - loss: 1.1275 - val_loss: 1.5166\n",
      "Epoch 9/20\n",
      "25841/25841 [==============================] - 598s 23ms/step - loss: 1.1063 - val_loss: 1.5161\n",
      "Epoch 10/20\n",
      "25841/25841 [==============================] - 624s 24ms/step - loss: 1.0724 - val_loss: 1.5133\n",
      "Epoch 11/20\n",
      "25841/25841 [==============================] - 627s 24ms/step - loss: 1.0427 - val_loss: 1.4905\n",
      "Epoch 12/20\n",
      "25841/25841 [==============================] - 613s 24ms/step - loss: 1.0226 - val_loss: 1.5479\n",
      "Epoch 13/20\n",
      "25841/25841 [==============================] - 636s 25ms/step - loss: 1.0024 - val_loss: 1.5307\n",
      "Epoch 14/20\n",
      "25841/25841 [==============================] - 633s 24ms/step - loss: 0.9789 - val_loss: 1.4768\n",
      "Epoch 15/20\n",
      "25841/25841 [==============================] - 625s 24ms/step - loss: 0.9221 - val_loss: 1.5068\n",
      "Epoch 17/20\n",
      "25841/25841 [==============================] - 611s 24ms/step - loss: 0.8981 - val_loss: 1.4750\n",
      "Epoch 18/20\n",
      "25841/25841 [==============================] - 606s 23ms/step - loss: 0.8769 - val_loss: 1.4803\n",
      "Epoch 19/20\n",
      "25841/25841 [==============================] - 614s 24ms/step - loss: 0.8580 - val_loss: 1.5044\n",
      "Epoch 20/20\n",
      "25841/25841 [==============================] - 608s 24ms/step - loss: 0.8348 - val_loss: 1.4977\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"lstm_log\")\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('lstm_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model_lstm.fit(train_inputs, train_outputs, callbacks=[tensorboard, earlystop, checkpoint], \n",
    "                              validation_split=0.05, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model_lstm.save(\"lstm.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE for CNN model : 1.684324740148957\n",
      "Test MSE for GRU model : 1.5355497767798483\n",
      "Test MSE for LSTM model : 1.4765458198950472\n"
     ]
    }
   ],
   "source": [
    "user_reviews = np.array(list(test_embedded.loc[:, \"userReviews\"]))\n",
    "movie_reviews = np.array(list(test_embedded.loc[:, \"movieReviews\"]))\n",
    "test_inputs = [user_reviews, movie_reviews]\n",
    "\n",
    "true_rating = np.array(list(test_embedded.loc[:, \"overall\"])).reshape((-1, 1))\n",
    "\n",
    "predictions_cnn = model_cnn.predict(test_inputs)\n",
    "predictions_gru = model_gru.predict(test_inputs)\n",
    "predictions_lstm = model_lstm.predict(test_inputs)\n",
    "\n",
    "error_cnn = np.square(predictions_cnn - true_rating)\n",
    "print(\"Test MSE for CNN model :\", np.average(error_cnn))\n",
    "\n",
    "error_gru = np.square(predictions_gru - true_rating)\n",
    "print(\"Test MSE for GRU model :\", np.average(error_gru))\n",
    "\n",
    "error_lstm = np.square(predictions_lstm - true_rating)\n",
    "print(\"Test MSE for LSTM model :\", np.average(error_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is very sparse and we need to run more epochs, and also a larger dataset. So the resulting MSE's are not so satisfying. However, we can still compare them and draw some early conclusions.\n",
    "\n",
    "1. RNN works better than CNN. A possible reason might be that reviews are sequential data.\n",
    "2. LSTM works better than GRU. More epochs will lead to better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
